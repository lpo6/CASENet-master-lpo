# import numpy as np
# from PIL import Image
# import os
# import zipfile
# import shutil
# import h5py
# from tqdm import tqdm
#
# import torch
#
# def convert_num_to_bitfield(label_data, h, w, npz_name, root_folder, h5_file, cls_num=19):
#     label_list = list(label_data)
#     all_bit_tensor_list = []
#     for n in label_list: # Iterate in each pixel
#         # Convert a value to binary format in each bit.
#         bitfield = np.asarray([int(digit) for digit in bin(n)[2:]])
#         bit_tensor = torch.from_numpy(bitfield)
#         actual_len = bit_tensor.size()[0]
#         padded_bit_tensor = torch.cat((torch.zeros(cls_num-actual_len).byte(), bit_tensor.byte()), dim=0)
#         all_bit_tensor_list.append(padded_bit_tensor)
#     all_bit_tensor_list = torch.stack(all_bit_tensor_list).view(h, w, cls_num)
#     h5_file.create_dataset('data/'+npz_name.replace('/', '_'), data=all_bit_tensor_list.numpy())
#
# if __name__ == "__main__":
#     f = open(r"D:\marterial\daerchuyanfwk_detection\TRY-2\CASENet-master/cityscapes-preprocess/data_proc/val.txt", 'r')
#     lines = f.readlines()
#     root_folder = r"D:\marterial\daerchuyanfwk_detection\TRY-2\CASENet-master/cityscapes-preprocess/data_proc/"
#
#     h5_file = h5py.File("val_label_binary_np.h5", 'w')
#     for ori_line in tqdm(lines):
#         line = ori_line.split()
#         bin_name = line[1]
#         img_name = line[0]
#
#         label_path = os.path.join(root_folder, bin_name)
#         img_path = os.path.join(root_folder, img_name)
#
#         img = Image.open(img_path).convert('RGB')
#         w, h = img.size # Notice: not h, w! This is very important! Otherwise, the label is wrong for each pixel.
#
#         label_data = np.fromfile(label_path, dtype=np.uint32)
#         npz_name = bin_name.replace("bin", "npy")
#         convert_num_to_bitfield(label_data, h, w, npz_name, root_folder, h5_file)

import os
import h5py
import numpy as np
from tqdm import tqdm
import time


def analyze_file_structure():
    """åˆ†æå®é™…çš„æ–‡ä»¶ç»“æ„"""
    data_proc = r'D:\marterial\daerchuyanfwk_detection\TRY-2\CASENet-master\cityscapes-preprocess\data_proc_small'

    print("ğŸ” åˆ†ææ–‡ä»¶ç»“æ„...")

    # æ£€æŸ¥gtFineç›®å½•çš„å®é™…ç»“æ„
    gtfine_dir = os.path.join(data_proc, 'gtFine')
    if os.path.exists(gtfine_dir):
        for split in ['train', 'val']:
            split_dir = os.path.join(gtfine_dir, split)
            if os.path.exists(split_dir):
                cities = os.listdir(split_dir)
                print(fr"\nğŸ“ {split}é›† - åŸå¸‚æ•°é‡: {len(cities)}")
                for city in cities[:3]:  # æ˜¾ç¤ºå‰3ä¸ªåŸå¸‚
                    city_dir = os.path.join(split_dir, city)
                    bin_files = [f for f in os.listdir(city_dir) if f.endswith('.bin')]
                    if bin_files:
                        print(f"   {city}: {len(bin_files)}ä¸ªbinæ–‡ä»¶")
                        print(f"     ç¤ºä¾‹: {bin_files[0]}")


def convert_dataset_working(data_proc, dataset_type):
    """
    çœŸæ­£å¯å·¥ä½œçš„HDF5è½¬æ¢ç‰ˆæœ¬
    """
    print(fr"\nğŸ¯ å¼€å§‹å¤„ç† {dataset_type} é›†...")

    list_file = os.path.join(data_proc, f'{dataset_type}.txt')
    hdf5_path = os.path.join(data_proc, f'{dataset_type}_label_binary_np_small.h5')

    if not os.path.exists(list_file):
        print(f"âŒ æ–‡ä»¶åˆ—è¡¨ä¸å­˜åœ¨: {list_file}")
        return 0

    with open(list_file, 'r') as f:
        lines = [line.strip() for line in f if line.strip()]

    print(f"ğŸ“Š æ€»æ•°æ®è¡Œæ•°: {len(lines)}")

    success_count = 0
    start_time = time.time()

    # åˆ é™¤å·²å­˜åœ¨çš„HDF5æ–‡ä»¶
    if os.path.exists(hdf5_path):
        os.remove(hdf5_path)

    with h5py.File(hdf5_path, 'w') as hf:
        for i, line in enumerate(tqdm(lines, desc=f"è½¬æ¢{dataset_type}")):
            try:
                parts = line.split()
                if not parts:
                    continue

                image_path = parts[0]

                if dataset_type == 'test':
                    # æµ‹è¯•é›† - åˆ›å»ºç©ºçš„å ä½ç¬¦æ•°æ®
                    placeholder = np.zeros((512,1024), dtype=np.uint32)
                    # ä½¿ç”¨ç®€å•é”®å
                    dataset_key = f"image_{i:06d}"
                    hf.create_dataset(dataset_key, data=placeholder, compression='gzip')
                    success_count += 1
                else:
                    # è®­ç»ƒé›†/éªŒè¯é›† - ä½¿ç”¨çœŸå®binæ•°æ®
                    # è§£æè·¯å¾„: /leftImg8bit/train/aachen/aachen_000000_000019_leftImg8bit.png
                    path_parts = image_path.split('/')
                    if len(path_parts) >= 4:
                        city_name = path_parts[3]  # aachen
                        file_base = path_parts[4].replace('_leftImg8bit.png', '')

                        # æ„å»ºæ­£ç¡®çš„binæ–‡ä»¶è·¯å¾„
                        bin_path = os.path.join(
                            data_proc,
                            'gtFine',
                            dataset_type,
                            city_name,
                            f'{file_base}_gtFine_edge.bin'
                        )

                        if os.path.exists(bin_path):
                            # è¯»å–binæ–‡ä»¶
                            with open(bin_path, 'rb') as bin_file:
                                binary_data = np.fromfile(bin_file, dtype=np.uint32)

                            # é‡å¡‘ä¸ºå›¾åƒå°ºå¯¸ (524288)
                            if binary_data.size == 524288:
                                image_data = binary_data.reshape(512,1024)
                            else:
                                # è®¡ç®—åˆé€‚çš„å°ºå¯¸
                                height = 512
                                width = binary_data.size // height
                                image_data = binary_data.reshape(height, width)

                            # ä½¿ç”¨ç®€å•é”®åä¿å­˜åˆ°HDF5
                            dataset_key = f"image_{i:06d}"
                            hf.create_dataset(dataset_key, data=image_data, compression='gzip')
                            success_count += 1

                            # æ˜¾ç¤ºå‰å‡ ä¸ªæˆåŠŸçš„æ–‡ä»¶
                            if success_count <= 3:
                                print(f"   âœ… æˆåŠŸè½¬æ¢: {os.path.basename(bin_path)} -> {dataset_key}")
                        else:
                            if i < 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯
                                print(f"   âŒ Binæ–‡ä»¶ä¸å­˜åœ¨: {bin_path}")
                    else:
                        if i < 5:
                            print(f"   âŒ è·¯å¾„æ ¼å¼é”™è¯¯: {image_path}")

            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥ (è¡Œ {i}): {e}")
                continue

    total_time = (time.time() - start_time) / 60
    file_size = os.path.getsize(hdf5_path) / (1024 * 1024) if os.path.exists(hdf5_path) else 0

    print(f"âœ… {dataset_type}é›†å®Œæˆ: {success_count}/{len(lines)} æ–‡ä»¶")
    print(f"ğŸ“ è¾“å‡ºå¤§å°: {file_size:.1f} MB")

    return success_count


def create_simple_solution():
    """
    æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼šç›´æ¥éå†gtFineç›®å½•
    """
    print(r"\nğŸ”„ å°è¯•ç®€å•è§£å†³æ–¹æ¡ˆ...")

    data_proc = r'D:\marterial\daerchuyanfwk_detection\TRY-2\CASENet-master\cityscapes-preprocess\data_proc_small'

    for dataset_type in ['train', 'val']:
        print(fr"\nğŸ¯ å¤„ç† {dataset_type} é›†...")

        hdf5_path = os.path.join(data_proc, f'{dataset_type}_label_binary_np_small.h5')
        gtfine_dir = os.path.join(data_proc, 'gtFine', dataset_type)

        if not os.path.exists(gtfine_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {gtfine_dir}")
            continue

        # åˆ é™¤å·²å­˜åœ¨çš„HDF5æ–‡ä»¶
        if os.path.exists(hdf5_path):
            os.remove(hdf5_path)

        success_count = 0
        with h5py.File(hdf5_path, 'w') as hf:
            # éå†æ‰€æœ‰åŸå¸‚ç›®å½•
            for city in os.listdir(gtfine_dir):
                city_dir = os.path.join(gtfine_dir, city)
                if os.path.isdir(city_dir):
                    # å¤„ç†æ‰€æœ‰binæ–‡ä»¶
                    bin_files = [f for f in os.listdir(city_dir) if f.endswith('_edge.bin')]

                    for bin_file in tqdm(bin_files, desc=f"å¤„ç†{city}"):
                        try:
                            bin_path = os.path.join(city_dir, bin_file)

                            # è¯»å–binæ–‡ä»¶
                            with open(bin_path, 'rb') as f:
                                binary_data = np.fromfile(f, dtype=np.uint32)

                            # é‡å¡‘ä¸º512x1024
                            if binary_data.size == 524288:
                                image_data = binary_data.reshape(512,1024)
                            else:
                                image_data = binary_data.reshape(512,1024)  # å¼ºåˆ¶é‡å¡‘

                            # ä¿å­˜åˆ°HDF5ï¼Œä½¿ç”¨åŸæ–‡ä»¶åä½œä¸ºé”®
                            dataset_key = bin_file.replace('.bin', '')
                            hf.create_dataset(dataset_key, data=image_data, compression='gzip')
                            success_count += 1

                        except Exception as e:
                            print(f"âŒ å¤„ç†å¤±è´¥ {bin_file}: {e}")
                            continue

        file_size = os.path.getsize(hdf5_path) / (1024 * 1024) if os.path.exists(hdf5_path) else 0
        print(f"âœ… {dataset_type}é›†å®Œæˆ: {success_count} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ è¾“å‡ºå¤§å°: {file_size:.1f} MB")


def verify_final_results():
    """éªŒè¯æœ€ç»ˆç»“æœ"""
    print(r"\nğŸ” éªŒè¯æœ€ç»ˆHDF5æ–‡ä»¶...")

    data_proc = r'D:\marterial\daerchuyanfwk_detection\TRY-2\CASENet-master\cityscapes-preprocess\data_proc_small'

    for dataset in ['train', 'val', 'test']:
        hdf5_path = os.path.join(data_proc, f'{dataset}_label_binary_np_small.h5')

        if os.path.exists(hdf5_path):
            size = os.path.getsize(hdf5_path) / (1024 * 1024)
            print(fr"\nğŸ“Š {dataset}: {size:.1f} MB")

            try:
                with h5py.File(hdf5_path, 'r') as hf:
                    keys = list(hf.keys())
                    print(f"   æ•°æ®é›†æ•°é‡: {len(keys)}")

                    if keys:
                        sample_key = keys[0]
                        sample_data = hf[sample_key][:]
                        print(f"   æ ·æœ¬å½¢çŠ¶: {sample_data.shape}")
                        print(f"   æ•°æ®ç±»å‹: {sample_data.dtype}")
                        print(f"   æ•°æ®èŒƒå›´: {sample_data.min()} ~ {sample_data.max()}")
            except Exception as e:
                print(f"   âŒ éªŒè¯å¤±è´¥: {e}")
        else:
            print(f"âŒ {dataset}: æ–‡ä»¶ä¸å­˜åœ¨")


def main():
    print("ğŸš€ å¯åŠ¨å®Œå…¨ä¿®å¤çš„HDF5è½¬æ¢")

    # å…ˆåˆ†æç»“æ„
    analyze_file_structure()

    # ä½¿ç”¨ç®€å•è§£å†³æ–¹æ¡ˆï¼ˆæ¨èï¼‰
    create_simple_solution()

    # éªŒè¯ç»“æœ
    verify_final_results()

    print(r"\nğŸ‰ æ‰€æœ‰è½¬æ¢å®Œæˆï¼")


if __name__ == '__main__':
    main()